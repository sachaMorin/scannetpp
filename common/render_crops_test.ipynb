{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    import renderpy\n",
    "except ImportError:\n",
    "    print(\"renderpy not installed. Please install renderpy from https://github.com/liu115/renderpy\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.colmap import read_model, write_model, Image\n",
    "from scene_release import ScannetppScene_Release\n",
    "from utils.utils import run_command, load_yaml_munch, load_json, read_txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file: /home/kumaraditya/scannetpp/common/configs/render.yml\n"
     ]
    }
   ],
   "source": [
    "p = argparse.ArgumentParser()\n",
    "p.add_argument(\"config_file\", help=\"Path to config file\", default=\"/home/kumaraditya/scannetpp/common/configs/render.yml\", nargs=\"?\")\n",
    "args = p.parse_args([])\n",
    "\n",
    "print(f\"Config file: {args.config_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_yaml_munch(args.config_file)\n",
    "\n",
    "# get the scenes to process\n",
    "if cfg.get(\"scene_ids\"):\n",
    "    scene_ids = cfg.scene_ids\n",
    "elif cfg.get(\"splits\"):\n",
    "    scene_ids = []\n",
    "    for split in cfg.splits:\n",
    "        split_path = Path(cfg.data_root) / \"splits\" / f\"{split}.txt\"\n",
    "        scene_ids += read_txt_list(split_path)\n",
    "\n",
    "output_dir = cfg.get(\"output_dir\")\n",
    "if output_dir is None:\n",
    "    # default to data folder in data_root\n",
    "    output_dir = Path(cfg.data_root) / \"data\"\n",
    "output_dir = Path(output_dir)\n",
    "\n",
    "render_devices = []\n",
    "if cfg.get(\"render_dslr\", False):\n",
    "    render_devices.append(\"dslr\")\n",
    "    raise Exception(\"This code is has not been tested with the DSLR data.\")\n",
    "if cfg.get(\"render_iphone\", False):\n",
    "    render_devices.append(\"iphone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0f5742640\n",
      "['iphone']\n"
     ]
    }
   ],
   "source": [
    "scene_id = scene_ids[0]\n",
    "print(scene_id)\n",
    "print(render_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init EGL\n",
      "Detected 5 devices\n",
      "Using device 0\n",
      "Using EGL version 1.5\n",
      "OpenGL version: 4.6.0 NVIDIA 550.107.02\n",
      "EGL version: 1.5\n",
      "Loaded mesh:MeshData:\n",
      "\tVertices:  917079\n",
      "\tColors:    917079\n",
      "\tNormals:   0\n",
      "\tTexCoords: 0\n",
      "\n",
      "\n",
      "Copy mesh to GPU: 917079 vertices, 1832073 faces\n",
      "Setup the frame and render buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering object crops using iphone images:   0%|          | 0/586 [00:00<?, ?it/s]/tmp/ipykernel_354731/1520543007.py:61: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  rgb = np.asarray(imageio.imread(iphone_rgb_path))\n",
      "Rendering object crops using iphone images: 100%|██████████| 586/586 [04:08<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from render_crops_utils import vert_to_obj_lookup, CropHeap, crop_rgb_mask, plot_grid_images\n",
    "\n",
    "scene = ScannetppScene_Release(scene_id, data_root=Path(cfg.data_root) / \"data\")\n",
    "render_engine = renderpy.Render()\n",
    "render_engine.setupMesh(str(scene.scan_mesh_path))\n",
    "\n",
    "# Load annotations\n",
    "segments_anno = json.load(open(scene.scan_anno_json_path, \"r\"))\n",
    "n_objects = len(segments_anno[\"segGroups\"])\n",
    "instance_colors = np.random.randint(low=0, high=256, size=(n_objects + 1, 3), dtype=np.uint8)\n",
    "instance_colors[0] = 255 # White bg\n",
    "vert_to_obj = vert_to_obj_lookup(segments_anno)\n",
    "\n",
    "# Crop heaps\n",
    "crop_heaps = dict()\n",
    "for obj in segments_anno[\"segGroups\"]:\n",
    "    crop_heaps[obj[\"id\"]] = dict()\n",
    "    crop_heaps[obj[\"id\"]][\"label\"] = obj[\"label\"]\n",
    "    crop_heaps[obj[\"id\"]][\"heap\"] = CropHeap(max_size=4)\n",
    "\n",
    "# Background class is 0\n",
    "assert 0 not in crop_heaps\n",
    "crop_heaps[0] = dict()\n",
    "crop_heaps[0][\"label\"] = \"BACKGROUND\"\n",
    "crop_heaps[0][\"heap\"] = CropHeap(max_size=4)\n",
    "\n",
    "\n",
    "for device in render_devices:\n",
    "    if device == \"dslr\":\n",
    "        cameras, images, points3D = read_model(scene.dslr_colmap_dir, \".txt\")\n",
    "    else:\n",
    "        cameras, images, points3D = read_model(scene.iphone_colmap_dir, \".txt\")\n",
    "    assert len(cameras) == 1, \"Multiple cameras not supported\"\n",
    "    camera = next(iter(cameras.values()))\n",
    "\n",
    "    fx, fy, cx, cy = camera.params[:4]\n",
    "    params = camera.params[4:]\n",
    "    camera_model = camera.model\n",
    "    render_engine.setupCamera(\n",
    "        camera.height, camera.width,\n",
    "        fx, fy, cx, cy,\n",
    "        camera_model,\n",
    "        params,      # Distortion parameters np.array([k1, k2, k3, k4]) or np.array([k1, k2, p1, p2])\n",
    "    )\n",
    "\n",
    "    near = cfg.get(\"near\", 0.05)\n",
    "    far = cfg.get(\"far\", 20.0)\n",
    "    rgb_dir = Path(cfg.output_dir) / scene_id / device / \"render_rgb\"\n",
    "    depth_dir = Path(cfg.output_dir) / scene_id / device / \"render_depth\"\n",
    "    # crop_dir = Path(cfg.output_dir) / scene_id / device / \"render_crops_kumar_w_sam\"\n",
    "    rgb_dir.mkdir(parents=True, exist_ok=True)\n",
    "    depth_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # crop_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for _, image in tqdm(images.items(), f\"Rendering object crops using {device} images\"):\n",
    "        world_to_camera = image.world_to_camera\n",
    "\n",
    "        rgb_rendered, _, vert_indices = render_engine.renderAll(world_to_camera, near, far)\n",
    "\n",
    "        iphone_rgb_path = Path(scene.iphone_rgb_dir) / image.name\n",
    "        rgb = np.asarray(imageio.imread(iphone_rgb_path))\n",
    "\n",
    "        vert_instance = vert_to_obj[vert_indices]\n",
    "        pix_instance = vert_instance[:, :, 0] # Some triangles actually belong to different objects. I don't think it will matter for crops.\n",
    "\n",
    "        # Visualize instances\n",
    "        # instance_rgb = instance_colors[pix_instance]\n",
    "        # imageio.imwrite(rgb_dir / image.name, instance_rgb)\n",
    "\n",
    "        objs = np.unique(pix_instance)\n",
    "\n",
    "        for obj in objs:\n",
    "            mask = pix_instance == obj\n",
    "            crop = crop_rgb_mask(rgb, rgb_rendered, mask, inflate_px=100)\n",
    "            crop_heaps[obj][\"heap\"].push(crop)\n",
    "\n",
    "\n",
    "        # instance_rgb = instance_rgb.astype(np.uint8)\n",
    "        # # Make depth in mm and clip to fit 16-bit image\n",
    "        # depth = (depth.astype(np.float32) * 1000).clip(0, 65535).astype(np.uint16)\n",
    "        # depth_name = image.name.split(\".\")[0] + \".png\"\n",
    "        # imageio.imwrite(depth_dir / depth_name, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, entry in tqdm(crop_heaps.items(), f\"Rendering image grids\"):\n",
    "#     heap = entry[\"heap\"]\n",
    "#     label = entry[\"label\"]\n",
    "#     if len(heap) and label.lower() not in [\n",
    "#         \"background\",\n",
    "#         \"wall\",\n",
    "#         \"floor\",\n",
    "#         \"ceiling\",\n",
    "#         \"split\",\n",
    "#         \"remove\",\n",
    "#     ]:\n",
    "#         crops = heap.get_sorted()\n",
    "#         rgbs = [c.rgb for c in crops]\n",
    "#         masks = [c.mask for c in crops]\n",
    "#         scores = [c.score for c in crops]\n",
    "#         plot_grid_images(\n",
    "#             rgbs + masks, grid_width=len(rgbs), title=entry[\"label\"]\n",
    "#         )\n",
    "#         plt.savefig(crop_dir / f\"{str(id).zfill(5)}.jpg\")\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log format\n",
    "    filename=\"sam2_video_mask_model.log\",  # Log file path\n",
    "    filemode=\"w\"  # 'w' to overwrite the log file each time, 'a' to append\n",
    ")\n",
    "\n",
    "class SAM2VideoMaskModel:\n",
    "    def __init__(self, sam2_checkpoint, model_cfg, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize SAM2 model and set device.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        if self.device.type == \"cuda\":\n",
    "            torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "        self.predictor = self._build_predictor(sam2_checkpoint, model_cfg)\n",
    "        self._initialize_storage()\n",
    "\n",
    "    def _build_predictor(self, sam2_checkpoint, model_cfg):\n",
    "        \"\"\"\n",
    "        Helper function to build the SAM2 video predictor.\n",
    "        \"\"\"\n",
    "        return build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=self.device)\n",
    "\n",
    "    def _initialize_storage(self):\n",
    "        \"\"\"\n",
    "        Initializes temporary directory and storage for RGB images, masks, and related data.\n",
    "        \"\"\"\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        logging.info(f\"Temporary directory created at: {self.temp_dir}\")\n",
    "\n",
    "        # Placeholder for resized images, masks, padded_masks, rgb_padded, padding information, and scores\n",
    "        self.rgb = []\n",
    "        self.rgb_padded = []\n",
    "        self.mask = []\n",
    "        self.masks_padded = []\n",
    "        self.masks_refined = []\n",
    "        self.padding_info = []\n",
    "        self.scores = []\n",
    "\n",
    "    def pad_and_store(self, rgbs, masks, scores):\n",
    "        \"\"\"\n",
    "        Pads the RGB images and masks to the size of the largest image and stores them.\n",
    "        \"\"\"\n",
    "        if len(rgbs) != len(masks) or len(rgbs) != len(scores):\n",
    "            raise ValueError(\"The number of RGB images, masks, and scores must match.\")\n",
    "\n",
    "        max_h, max_w = self._get_max_dimensions(rgbs)\n",
    "\n",
    "        for idx, (rgb, mask, score) in enumerate(zip(rgbs, masks, scores)):\n",
    "            padded_rgb, padded_mask, padding_info = self._pad_image_and_mask(rgb, mask, max_h, max_w)\n",
    "            self._store_padded_data(idx, rgb, padded_rgb, mask, padded_mask, score, padding_info)\n",
    "\n",
    "    def _get_max_dimensions(self, rgbs):\n",
    "        \"\"\"\n",
    "        Get the maximum height and width among the provided RGB images.\n",
    "        \"\"\"\n",
    "        max_h = max([rgb.shape[0] for rgb in rgbs])\n",
    "        max_w = max([rgb.shape[1] for rgb in rgbs])\n",
    "        return max_h, max_w\n",
    "\n",
    "    def _pad_image_and_mask(self, rgb, mask, max_h, max_w):\n",
    "        \"\"\"\n",
    "        Pads the RGB and mask to the provided max dimensions.\n",
    "        \"\"\"\n",
    "        h, w, _ = rgb.shape\n",
    "        pad_h, pad_w = max_h - h, max_w - w\n",
    "        padding_info = ((0, pad_h), (0, pad_w))\n",
    "\n",
    "        padded_rgb = np.pad(rgb, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"constant\", constant_values=0)\n",
    "        padded_mask = np.pad(mask, ((0, pad_h), (0, pad_w)), mode=\"constant\", constant_values=0)\n",
    "\n",
    "        return padded_rgb, padded_mask, padding_info\n",
    "\n",
    "    def _store_padded_data(self, idx, rgb, padded_rgb, mask, padded_mask, score, padding_info):\n",
    "        \"\"\"\n",
    "        Store padded data, and save the padded RGB image to the temp directory.\n",
    "        \"\"\"\n",
    "        rgb_filename = os.path.join(self.temp_dir, f\"{idx}.jpg\")\n",
    "        cv2.imwrite(rgb_filename, padded_rgb)\n",
    "\n",
    "        self.rgb.append(rgb)\n",
    "        self.rgb_padded.append(padded_rgb)\n",
    "        self.mask.append(mask)\n",
    "        self.masks_padded.append(padded_mask)\n",
    "        self.scores.append(score)\n",
    "        self.padding_info.append(padding_info)\n",
    "\n",
    "    def set_state_and_refine_masks(self):\n",
    "        \"\"\"\n",
    "        Set the state for the SAM2 predictor and refine masks.\n",
    "        \"\"\"\n",
    "        inference_state = self._initialize_inference_state()\n",
    "        points, labels, highest_score_idx = self._get_initial_points()\n",
    "\n",
    "        self._refine_masks(inference_state, points, labels, highest_score_idx)\n",
    "        self.predictor.reset_state(inference_state)\n",
    "\n",
    "    def _initialize_inference_state(self):\n",
    "        \"\"\"\n",
    "        Initialize the predictor's inference state.\n",
    "        \"\"\"\n",
    "        return self.predictor.init_state(video_path=self.temp_dir)\n",
    "\n",
    "    def _get_initial_points(self):\n",
    "        \"\"\"\n",
    "        Determine initial points based on the mask with the highest score.\n",
    "        \"\"\"\n",
    "        highest_score_idx = np.argmax(self.scores)\n",
    "        highest_score_mask = self.masks_padded[highest_score_idx]\n",
    "\n",
    "        mask_indices = np.argwhere(highest_score_mask > 0)\n",
    "        points = np.array([mask_indices[np.random.choice(len(mask_indices))] for _ in range(5)], dtype=np.float32)\n",
    "        labels = np.ones(5, dtype=np.int32)\n",
    "\n",
    "        return points, labels, highest_score_idx\n",
    "\n",
    "    def _refine_masks(self, inference_state, points, labels, highest_score_idx):\n",
    "        \"\"\"\n",
    "        Refine the masks and propagate through frames.\n",
    "        \"\"\"\n",
    "        _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=highest_score_idx,\n",
    "            obj_id=1,\n",
    "            points=points,\n",
    "            labels=labels,\n",
    "            box=None,\n",
    "        )\n",
    "\n",
    "        for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(inference_state):\n",
    "            refined_mask = (out_mask_logits[0] > 0.0).cpu().numpy().squeeze()\n",
    "            self.masks_refined.append(refined_mask)\n",
    "\n",
    "    def unpad_masks_to_original_size(self):\n",
    "        \"\"\"\n",
    "        Remove padding from refined masks to restore them to their original size.\n",
    "        \"\"\"\n",
    "        self.masks_refined = [self._unpad_mask(mask, frame_idx) for frame_idx, mask in enumerate(self.masks_refined)]\n",
    "\n",
    "    def _unpad_mask(self, mask, frame_idx):\n",
    "        \"\"\"\n",
    "        Unpad a single mask based on its padding information.\n",
    "        \"\"\"\n",
    "        pad_h, pad_w = self.padding_info[frame_idx]\n",
    "        if pad_h[1] != 0 or pad_w[1] != 0:\n",
    "            return mask[:-pad_h[1], :-pad_w[1]]\n",
    "        return mask\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Clean up the temporary directory and clear stored data.\n",
    "        \"\"\"\n",
    "        self._clear_temp_directory()\n",
    "        self._clear_storage()\n",
    "\n",
    "    def _clear_temp_directory(self):\n",
    "        \"\"\"\n",
    "        Clears the contents of the temporary directory but leaves the directory itself.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.temp_dir):\n",
    "            for filename in os.listdir(self.temp_dir):\n",
    "                file_path = os.path.join(self.temp_dir, filename)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                        os.unlink(file_path)  # Remove file or symbolic link\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)  # Remove subdirectory and its contents\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "            \n",
    "            logging.info(f\"Contents of temporary directory {self.temp_dir} removed.\")\n",
    "\n",
    "    def _clear_storage(self):\n",
    "        \"\"\"\n",
    "        Clears all stored data except for the SAM2 model.\n",
    "        \"\"\"\n",
    "        self.rgb = []\n",
    "        self.rgb_padded = []\n",
    "        self.mask = []\n",
    "        self.masks_padded = []\n",
    "        self.scores = []\n",
    "        self.padding_info = []\n",
    "        self.masks_refined = []\n",
    "        logging.info(\"Cleared all stored images, masks, scores, and padding information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_checkpoint = \"/home/kumaraditya/checkpoints/sam2_hiera_large.pt\"\n",
    "sam2_model_cfg = \"sam2_hiera_l.yaml\"\n",
    "# sam2_video_model = SAM2VideoMaskModel(sam2_checkpoint, sam2_model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, entry in tqdm(crop_heaps.items(), f\"Rendering image grids\"):\n",
    "#     heap = entry[\"heap\"]\n",
    "#     label = entry[\"label\"]\n",
    "#     if len(heap) and label.lower() not in [\n",
    "#         \"background\",\n",
    "#         \"wall\",\n",
    "#         \"floor\",\n",
    "#         \"ceiling\",\n",
    "#         \"split\",\n",
    "#         \"remove\",\n",
    "#     ]:\n",
    "#         crops = heap.get_sorted()\n",
    "#         rgbs = [c.rgb for c in crops]\n",
    "#         masks = [c.mask for c in crops]\n",
    "#         scores = [c.score for c in crops]\n",
    "\n",
    "#         sam2_video_model.pad_and_store(rgbs, masks, scores)\n",
    "#         sam2_video_model.set_state_and_refine_masks()\n",
    "#         sam2_video_model.unpad_masks_to_original_size()\n",
    "#         masks = sam2_video_model.masks_refined\n",
    "#         sam2_video_model.cleanup()\n",
    "        \n",
    "#         plot_grid_images(\n",
    "#             rgbs + masks, grid_width=len(rgbs), title=entry[\"label\"]\n",
    "#         )\n",
    "#         plt.savefig(crop_dir / f\"{str(id).zfill(5)}.jpg\")\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "class SAM2ImageMaskModel:\n",
    "    def __init__(self, sam2_checkpoint, model_cfg, device=\"cuda\", num_points=5, ransac_iterations=10):\n",
    "        \"\"\"\n",
    "        Initialize SAM2 model, set device, and configure number of points and RANSAC iterations.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.num_points = num_points  # Number of points to sample from the mask\n",
    "        self.ransac_iterations = ransac_iterations  # Number of RANSAC iterations\n",
    "\n",
    "        if self.device.type == \"cuda\":\n",
    "            torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "            if torch.cuda.get_device_properties(0).major >= 8:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "        self.predictor = self._build_predictor(sam2_checkpoint, model_cfg)\n",
    "        self._initialize_storage()\n",
    "\n",
    "    def _build_predictor(self, sam2_checkpoint, model_cfg):\n",
    "        \"\"\"\n",
    "        Helper function to build the SAM2 image predictor.\n",
    "        \"\"\"\n",
    "        sam2 = build_sam2(model_cfg, sam2_checkpoint, device=self.device)\n",
    "        return SAM2ImagePredictor(sam2)\n",
    "\n",
    "    def _initialize_storage(self):\n",
    "        \"\"\"\n",
    "        Initializes storage for RGB images, masks, and related data.\n",
    "        \"\"\"\n",
    "        self.rgb = []\n",
    "        self.mask = []\n",
    "        self.masks_refined = []\n",
    "        self.crop_scores = []\n",
    "        self.sam_scores = []\n",
    "\n",
    "    def store_data(self, rgbs, masks, scores):\n",
    "        \"\"\"\n",
    "        Stores the RGB images, masks, and scores.\n",
    "        \"\"\"\n",
    "        if len(rgbs) != len(masks) or len(rgbs) != len(scores):\n",
    "            raise ValueError(\"The number of RGB images, masks, and scores must match.\")\n",
    "\n",
    "        for idx, (rgb, mask, score) in enumerate(zip(rgbs, masks, scores)):\n",
    "            self._store_data(idx, rgb, mask, score)\n",
    "\n",
    "    def _store_data(self, idx, rgb, mask, score):\n",
    "        \"\"\"\n",
    "        Store RGB, mask, and score data.\n",
    "        \"\"\"\n",
    "        self.rgb.append(rgb)\n",
    "        self.mask.append(mask)\n",
    "        self.crop_scores.append(score)\n",
    "\n",
    "    def _sample_points_from_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sample n points from the provided mask where the mask is non-zero.\n",
    "        \"\"\"\n",
    "        mask_indices = np.argwhere(mask > 0)  # Get non-zero mask points\n",
    "        if len(mask_indices) == 0:\n",
    "            raise ValueError(\"No valid mask points to sample from.\")\n",
    "\n",
    "        # Randomly sample 'n' points\n",
    "        sampled_points = np.array(random.choices(mask_indices, k=self.num_points), dtype=np.float32)\n",
    "        return sampled_points\n",
    "\n",
    "    def _set_image_for_predictor(self, rgb):\n",
    "        \"\"\"\n",
    "        Preprocesses the RGB image and sets it for the SAM predictor.\n",
    "        \"\"\"\n",
    "        # # Convert to RGB format (OpenCV loads images in BGR by default)\n",
    "        # rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Set the image for the SAM predictor\n",
    "        self.predictor.set_image(rgb)\n",
    "\n",
    "    def _predict_mask_with_points(self, rgb, points):\n",
    "        \"\"\"\n",
    "        Use sampled points to prompt SAM for mask prediction and return mask and score.\n",
    "        \"\"\"\n",
    "        labels = np.ones(len(points), dtype=np.int32)  # All positive labels (foreground)\n",
    "        \n",
    "        # Set the current RGB image for the predictor\n",
    "        self._set_image_for_predictor(rgb)\n",
    "\n",
    "        # Perform mask prediction using the points and labels\n",
    "        masks, scores, _ = self.predictor.predict(\n",
    "            point_coords=points,\n",
    "            point_labels=labels,\n",
    "            multimask_output=False\n",
    "        )\n",
    "\n",
    "        # Convert mask to bool type if it's not already\n",
    "        if masks.dtype != bool:\n",
    "            masks = masks.astype(bool)\n",
    "\n",
    "        return masks, scores\n",
    "\n",
    "    def _predict_mask_with_points_and_bbox(self, rgb, points, bbox):\n",
    "        \"\"\"\n",
    "        Use sampled points to prompt SAM for mask prediction and return mask and score.\n",
    "        \"\"\"\n",
    "        labels = np.ones(len(points), dtype=np.int32)  # All positive labels (foreground)\n",
    "        \n",
    "        # Set the current RGB image for the predictor\n",
    "        self._set_image_for_predictor(rgb)\n",
    "\n",
    "        # Perform mask prediction using the points and labels\n",
    "        masks, scores, _ = self.predictor.predict(\n",
    "            point_coords=points,\n",
    "            point_labels=labels,\n",
    "            box=bbox[None, :],\n",
    "            multimask_output=False\n",
    "        )\n",
    "\n",
    "        # Convert mask to bool type if it's not already\n",
    "        if masks.dtype != bool:\n",
    "            masks = masks.astype(bool)\n",
    "\n",
    "        return masks, scores\n",
    "\n",
    "    def ransac_mask_selection(self):\n",
    "        \"\"\"\n",
    "        Perform RANSAC-like sampling of points and select the best mask based on SAM score.\n",
    "        \"\"\"\n",
    "        for idx, (rgb, mask) in enumerate(zip(self.rgb, self.mask)):\n",
    "            best_score = -float('inf')\n",
    "            best_mask = None\n",
    "\n",
    "            for _ in range(self.ransac_iterations):\n",
    "                try:\n",
    "                    # Sample points from the mask\n",
    "                    sampled_points = self._sample_points_from_mask(mask)\n",
    "                    bbox = self._get_bounding_box_from_mask(mask)\n",
    "\n",
    "                    # Get mask prediction and score from SAM\n",
    "                    # predicted_mask, predicted_scores = self._predict_mask_with_points(rgb, sampled_points)\n",
    "                    predicted_mask, predicted_scores = self._predict_mask_with_points_and_bbox(rgb, sampled_points, bbox)\n",
    "\n",
    "                    predicted_mask = predicted_mask[0]  # Assuming single mask is returned\n",
    "                        \n",
    "                    # Choose the mask with the highest score\n",
    "                    score = predicted_scores[0]  # Assuming single mask is returned\n",
    "                    # score = self._mask_score_calculation(idx, predicted_mask, predicted_scores[0])\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_mask = predicted_mask\n",
    "                except ValueError as e:\n",
    "                    logging.warning(f\"Skipping frame {idx} due to error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Store the best mask and score for the current frame\n",
    "            self.masks_refined.append(best_mask)\n",
    "            self.sam_scores.append(best_score)\n",
    "\n",
    "    def _get_bounding_box_from_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Given a binary mask, return the bounding box in xyxy format.\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the indices where the mask is True (non-zero)\n",
    "        rows, cols = np.where(mask)\n",
    "\n",
    "        # If the mask is empty (no True values), return an empty bounding box\n",
    "        if len(rows) == 0 or len(cols) == 0:\n",
    "            return [0, 0, 0, 0]\n",
    "        \n",
    "        # Get the bounding box coordinates\n",
    "        x_min = np.min(cols)\n",
    "        y_min = np.min(rows)\n",
    "        x_max = np.max(cols)\n",
    "        y_max = np.max(rows)\n",
    "\n",
    "        return np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "    def _mask_score_calculation(self, idx, refined_mask, sam_score):\n",
    "        current_mask = self.mask[idx]\n",
    "\n",
    "        current_area = np.sum(current_mask)\n",
    "        refined_area = np.sum(refined_mask)\n",
    "        areas_diff = abs(current_area - refined_area)\n",
    "\n",
    "        areas_score = 1 / (1 + areas_diff)\n",
    "\n",
    "        final_score = areas_score * sam_score\n",
    "        return final_score\n",
    "\n",
    "    def refine_masks(self):\n",
    "        \"\"\"\n",
    "        Public method to trigger RANSAC-based mask refinement.\n",
    "        \"\"\"\n",
    "        self.ransac_mask_selection()\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Cleans up the stored data, including RGBs, masks, and scores, \n",
    "        while keeping the SAM model ready for further use.\n",
    "        \"\"\"\n",
    "        # Clear all stored data (RGBs, masks, scores)\n",
    "        self.rgb = []\n",
    "        self.mask = []\n",
    "        self.masks_refined = []\n",
    "        self.crop_scores = []\n",
    "        self.sam_scores = []\n",
    "\n",
    "        logging.info(\"Cleared all stored images, masks, scores, and refined masks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_img_model = SAM2ImageMaskModel(sam2_checkpoint, sam2_model_cfg, num_points=3, ransac_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering image grids: 100%|██████████| 135/135 [14:47<00:00,  6.57s/it]\n"
     ]
    }
   ],
   "source": [
    "crop_dir = Path(cfg.output_dir) / scene_id / device / \"render_crops_sam2_img\"\n",
    "crop_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for id, entry in tqdm(crop_heaps.items(), f\"Rendering image grids\"):\n",
    "    heap = entry[\"heap\"]\n",
    "    label = entry[\"label\"]\n",
    "    if len(heap) and label.lower() not in [\n",
    "        \"background\",\n",
    "        \"wall\",\n",
    "        \"floor\",\n",
    "        \"ceiling\",\n",
    "        \"split\",\n",
    "        \"remove\",\n",
    "    ]:\n",
    "        crops = heap.get_sorted()\n",
    "        rgbs = [c.rgb for c in crops]\n",
    "        masks = [c.mask for c in crops]\n",
    "        scores = [c.score for c in crops]\n",
    "\n",
    "        sam2_img_model.store_data(rgbs, masks, scores)\n",
    "        sam2_img_model.refine_masks()\n",
    "        masks = sam2_img_model.masks_refined\n",
    "        sam2_img_model.cleanup()\n",
    "        \n",
    "        plot_grid_images(\n",
    "            rgbs, masks, grid_width=len(rgbs), title=entry[\"label\"]\n",
    "        )\n",
    "        plt.savefig(crop_dir / f\"{str(id).zfill(5)}.jpg\")\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scannetpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
